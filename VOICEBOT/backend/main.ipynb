{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKQ6mGDCT8dA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "import librosa\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "SAMPLE_RATE = 16000\n",
        "gemini_api_key = 'GEMINI_API_KEY'"
      ],
      "metadata": {
        "id": "BnzZL50zVZMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"distil-whisper/distil-large-v3\"\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "k4UmS_ceVj-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_multichannel_audio(audio_file, channel):\n",
        "  # we suppose tha the audio is multichannel the agent is in separated channel to the client\n",
        "  waveform,sample_rate = librosa.load(audio_file, mono=False)\n",
        "  if sample_rate != 16000:\n",
        "    waveform = librosa.resample(waveform, sample_rate, 16000)\n",
        "  return waveform[channel], sample_rate"
      ],
      "metadata": {
        "id": "N-aCD-iiVtW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transcript(audio_file, channel):\n",
        "  # transcribe the audio\n",
        "  waveform,sample_rate = load_multichannel_audio(audio_file, channel)\n",
        "  result = pipe(waveform, return_timestamps=True, batch_size=4)\n",
        "  return result['text']\n",
        "  return"
      ],
      "metadata": {
        "id": "nzq_JpMqVtZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "\n",
        "\n",
        "```\n",
        "\"text\": \"Bonjour, je vous appelle parce que je suis vraiment mécontent\n",
        "de ma dernière commande. Je suis un client fidèle depuis plusieurs années,\n",
        "mais là, je trouve que le service est vraiment inacceptable.\",\n",
        "\"timestamp\": 86,\n",
        "\"duration\": 19,\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VMCHOWgjO9m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_discussion(audio_file):\n",
        "  # get transcript per channel\n",
        "  _, client_transcript = get_transcript(audio_file, 0)\n",
        "  for chunk in client_transcript:\n",
        "    chunk['channel'] = 'Client'\n",
        "  _, agent_transcript = get_transcript(audio_file, 1)\n",
        "  for chunk in agent_transcript:\n",
        "    chunk['channel'] = 'Agent'\n",
        "\n",
        "  #Combine the transcript and sort them\n",
        "  combined_transcript = client_transcript + agent_transcript\n",
        "  combined_transcript.sort(key=lambda x: x['timestamp'])\n",
        "  transcript = []\n",
        "  # recuperate the transcript in the ordre\n",
        "  for chunk in combined_transcript:\n",
        "    if chunk['channel'] == 'Client':\n",
        "      transcript.append(f'Client: {chunk[\"text\"]}')\n",
        "    else:\n",
        "      transcript.append(f'Agent: {chunk[\"text\"]}')\n",
        "    transcript.append(f'Agent: {chunk[\"text\"]}')\n",
        "  return '\\n'.join(transcript)"
      ],
      "metadata": {
        "id": "ETpWVLnaMGyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "\n",
        "\n",
        "```\n",
        "\"speaker\": \"Client\",\n",
        "        \"text\": \"Bonjour, je vous appelle parce que je suis vraiment mécontent\n",
        "        de ma dernière commande. Je suis un client fidèle depuis plusieurs années,\n",
        "        mais là, je trouve que le service est vraiment inacceptable.\",\n",
        "        \"timestamps\": 86,\n",
        "        \"duration\": 19,\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "va6WhLC1PND_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_summary(transcript):\n",
        "  #Import the prompt\n",
        "  with open('prompt_summary.txt', 'r', encoding='utf-f') as prompt_file:\n",
        "    summary_prompt = prompt_file.read()\n",
        "  prompt = summary_prompt + f' voici la discussion : {transcript}'\n",
        "  headers = {\n",
        "        \"Authorization\": f\"Bearer {gemini_api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "  data = {\n",
        "        \"model\": \"gemini-pro\",\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": 0.05,  # deterministic answers\n",
        "        \"max_tokens\": 256  #\n",
        "    }\n",
        "  # Call the API\n",
        "  response = requests.post(\"https://api.google.com/v1/chat/completions\", headers=headers, json=data)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "        return response.json()[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "O26MyLzeML1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_and_tags(transcript):\n",
        "  #Import the prompt\n",
        "  with open('prompt_semtiment_tags.txt', 'r', encoding='utf-f') as prompt_file:\n",
        "    sentiment_prompt = prompt_file.read()\n",
        "  prompt = sentiment_prompt + f' voici la discussion : {transcript}'\n",
        "  headers = {\n",
        "        \"Authorization\": f\"Bearer {gemini_api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "  data = {\n",
        "        \"model\": \"gemini-pro\",\n",
        "        \"prompt\": prompt,\n",
        "        \"temperature\": 0.05,  # deterministic answers\n",
        "        \"max_tokens\": 256  #\n",
        "    }\n",
        "  # Call the API\n",
        "  response = requests.post(\"https://api.google.com/v1/chat/completions\", headers=headers, json=data)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "        return response.json()[\"choices\"][0][\"text\"]\n",
        "  return"
      ],
      "metadata": {
        "id": "hLoxDd4dMR_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output\n",
        "\n",
        "\n",
        "```\n",
        "\"speaker\": \"Client\",\n",
        "        \"text\": \"Bonjour, je vous appelle parce que je suis vraiment mécontent\n",
        "        de ma dernière commande. Je suis un client fidèle depuis plusieurs années,\n",
        "        mais là, je trouve que le service est vraiment inacceptable.\",\n",
        "        \"timestamp\": 86,\n",
        "        \"duration\": 19,\n",
        "        \"sentiment\": \"negative\",\n",
        "        \"tag\": \"client non satisfé\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "f0F_Gnm8OSAF"
      }
    }
  ]
}